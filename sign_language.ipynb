{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSign Language Recognition Pipeline\\n\\nThis script implements a complete pipeline for sign language recognition including:\\n1. Data Loading and Preprocessing\\n2. Data Analysis \\n3. Model Training\\n4. Cross-Validation\\n5. Evaluation\\n\\nAvailable architectures:\\n- CNN-LSTM (src/models/cnn_lstm.py)\\n- Video Transformer (src/models/video_transformer.py)\\n- I3D Model (WLASL/code/I3D/pytorch_i3d.py)\\n- TGCN Model (WLASL/code/TGCN/tgcn_model.py)\\n- EfficientSignNet (wlasl_modified/src/models/efficient_sign_net.py)\\n- Hybrid Models (wlasl_modified/src/models/hybrid_transformers.py):\\n  * CNN-Transformer\\n  * TimeSformer\\n\\nMemory Management Tips:\\n1. Train one model at a time\\n2. Use appropriate batch sizes (start with 8)\\n3. Enable gradient checkpointing for transformer models\\n4. Clean up memory between training runs\\n5. Monitor GPU memory usage\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sign Language Recognition Pipeline\n",
    "\n",
    "This script implements a complete pipeline for sign language recognition including:\n",
    "1. Data Loading and Preprocessing\n",
    "2. Data Analysis \n",
    "3. Model Training\n",
    "4. Cross-Validation\n",
    "5. Evaluation\n",
    "\n",
    "Available architectures:\n",
    "- CNN-LSTM (src/models/cnn_lstm.py)\n",
    "- Video Transformer (src/models/video_transformer.py)\n",
    "- I3D Model (WLASL/code/I3D/pytorch_i3d.py)\n",
    "- TGCN Model (WLASL/code/TGCN/tgcn_model.py)\n",
    "- EfficientSignNet (wlasl_modified/src/models/efficient_sign_net.py)\n",
    "- Hybrid Models (wlasl_modified/src/models/hybrid_transformers.py):\n",
    "  * CNN-Transformer\n",
    "  * TimeSformer\n",
    "\n",
    "Memory Management Tips:\n",
    "1. Train one model at a time\n",
    "2. Use appropriate batch sizes (start with 8)\n",
    "3. Enable gradient checkpointing for transformer models\n",
    "4. Clean up memory between training runs\n",
    "5. Monitor GPU memory usage\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:Some models could not be imported: cannot import name 'MODEL_CONFIG' from 'wlasl_modified.src.config' (/media/yasir/D/sign language/wlasl_modified/src/config.py)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Standard imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Import model architectures\n",
    "try:\n",
    "    from src.models.cnn_lstm import SignLanguageCNNLSTM, CNNLSTMConfig\n",
    "    from src.models.video_transformer import VideoTransformer, TransformerConfig\n",
    "    from WLASL.code.I3D.pytorch_i3d import InceptionI3d\n",
    "    from wlasl_modified.src.models.efficient_sign_net import EfficientSignNet\n",
    "    from wlasl_modified.src.models.hybrid_transformers import CNNTransformer, TimeSformer\n",
    "except ImportError as e:\n",
    "    logger.warning(f\"Some models could not be imported: {e}\")\n",
    "\n",
    "from wlasl_modified.src.data.preprocessing import MemoryEfficientPreprocessor\n",
    "from wlasl_modified.src.data.loader import create_data_loaders, MemoryEfficientDataset\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"Handle data loading and preprocessing.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: Path, processed_dir: Path):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.processed_dir = Path(processed_dir)\n",
    "        self.preprocessor = MemoryEfficientPreprocessor(\n",
    "            output_dir=processed_dir / 'frames',\n",
    "            frame_size=(224, 224),  # Standard size for most models\n",
    "            target_fps=25,  # Standard frame rate\n",
    "            chunk_size=32   # Process videos in chunks to save memory\n",
    "        )\n",
    "    \n",
    "    def preprocess_videos(self):\n",
    "        \"\"\"Preprocess all videos in data directory.\"\"\"\n",
    "        # Create output directory if needed\n",
    "        self.processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Get all video files\n",
    "        video_paths = list(self.data_dir.glob('**/*.mp4'))\n",
    "        logger.info(f\"Found {len(video_paths)} videos\")\n",
    "        \n",
    "        # Process each video\n",
    "        results = []\n",
    "        for video_path in tqdm(video_paths, desc=\"Processing videos\"):\n",
    "            try:\n",
    "                result = self.preprocessor.preprocess_video(video_path)\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {video_path}: {str(e)}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def create_dataloaders(self, batch_size: int = 8, num_workers: int = 4):\n",
    "        \"\"\"Create train/val/test dataloaders.\"\"\"\n",
    "        # Load data info\n",
    "        with open(self.processed_dir / 'preprocessing_results.json', 'r') as f:\n",
    "            data_info = json.load(f)\n",
    "\n",
    "        return create_data_loaders(\n",
    "            data_info=data_info,\n",
    "            processed_dir=self.processed_dir / 'frames',\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataAnalyzer:\n",
    "    \"\"\"Analyze processed dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, processed_dir: Path):\n",
    "        self.processed_dir = Path(processed_dir)\n",
    "    \n",
    "    def analyze_dataset(self):\n",
    "        \"\"\"Analyze dataset statistics.\"\"\"\n",
    "        frame_dirs = list((self.processed_dir / 'frames').glob('*'))\n",
    "        \n",
    "        stats = {\n",
    "            'num_samples': len(frame_dirs),\n",
    "            'frames_per_video': [],\n",
    "            'video_sizes': [],\n",
    "            'class_distribution': {}\n",
    "        }\n",
    "        \n",
    "        for frame_dir in frame_dirs:\n",
    "            # Count frames\n",
    "            frames = list(frame_dir.glob('*.jpg'))\n",
    "            stats['frames_per_video'].append(len(frames))\n",
    "            \n",
    "            # Calculate video size\n",
    "            size = sum(f.stat().st_size for f in frames) / 1024**2  # MB\n",
    "            stats['video_sizes'].append(size)\n",
    "            \n",
    "            # Update class distribution\n",
    "            class_name = frame_dir.name.split('_')[0]\n",
    "            stats['class_distribution'][class_name] = \\\n",
    "                stats['class_distribution'].get(class_name, 0) + 1\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def plot_statistics(self, stats: dict):\n",
    "        \"\"\"Plot dataset statistics.\"\"\"\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Frames per video distribution\n",
    "        sns.histplot(stats['frames_per_video'], ax=ax1)\n",
    "        ax1.set_title('Frames per Video Distribution')\n",
    "        ax1.set_xlabel('Number of Frames')\n",
    "        \n",
    "        # Video sizes distribution\n",
    "        sns.histplot(stats['video_sizes'], ax=ax2)\n",
    "        ax2.set_title('Video Sizes Distribution')\n",
    "        ax2.set_xlabel('Size (MB)')\n",
    "        \n",
    "        # Class distribution\n",
    "        class_dist = pd.Series(stats['class_distribution']).sort_values(ascending=False)\n",
    "        class_dist.plot(kind='bar', ax=ax3)\n",
    "        ax3.set_title('Class Distribution')\n",
    "        ax3.set_xlabel('Class')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Basic statistics\n",
    "        ax4.axis('off')\n",
    "        stats_text = (\n",
    "            f\"Total samples: {stats['num_samples']}\\n\"\n",
    "            f\"Number of classes: {len(stats['class_distribution'])}\\n\"\n",
    "            f\"Avg frames per video: {np.mean(stats['frames_per_video']):.1f}\\n\"\n",
    "            f\"Avg video size: {np.mean(stats['video_sizes']):.1f} MB\"\n",
    "        )\n",
    "        ax4.text(0.1, 0.5, stats_text, fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"Handle model training and evaluation.\"\"\"\n",
    "    \n",
    "    AVAILABLE_MODELS = {\n",
    "        'cnn_lstm': 'CNN-LSTM model with ResNet backbone',\n",
    "        'transformer': 'Video Transformer with self-attention',\n",
    "        'i3d': 'Inflated 3D ConvNet (I3D)',\n",
    "        'tgcn': 'Temporal Graph Convolutional Network',\n",
    "        'efficient': 'Memory-efficient Sign Language Network',\n",
    "        'cnn_transformer': 'Hybrid CNN-Transformer',\n",
    "        'timesformer': 'Time-Space Transformer'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_name: str,\n",
    "                 num_classes: int,\n",
    "                 device: torch.device = None):\n",
    "        if model_name not in self.AVAILABLE_MODELS:\n",
    "            raise ValueError(\n",
    "                f\"Unknown model: {model_name}. Available models: {list(self.AVAILABLE_MODELS.keys())}\"\n",
    "            )\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Create model\n",
    "        self.model = self._create_model()\n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "        # Training components\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters())\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', patience=5\n",
    "        )\n",
    "    \n",
    "    def _create_model(self):\n",
    "        \"\"\"Create model based on specified architecture.\"\"\"\n",
    "        if self.model_name == 'cnn_lstm':\n",
    "            config = CNNLSTMConfig(num_classes=self.num_classes)\n",
    "            return SignLanguageCNNLSTM(config)\n",
    "        \n",
    "        elif self.model_name == 'transformer':\n",
    "            config = TransformerConfig(num_classes=self.num_classes)\n",
    "            return VideoTransformer(config)\n",
    "        \n",
    "        elif self.model_name == 'i3d':\n",
    "            return InceptionI3d(\n",
    "                num_classes=self.num_classes,\n",
    "                in_channels=3\n",
    "            )\n",
    "        \n",
    "        elif self.model_name == 'tgcn':\n",
    "            return TGCN(\n",
    "                num_classes=self.num_classes,\n",
    "                in_channels=3,\n",
    "                graph_args={'layout': 'openpose', 'strategy': 'spatial'}\n",
    "            )\n",
    "        \n",
    "        elif self.model_name == 'efficient':\n",
    "            return EfficientSignNet(\n",
    "                num_classes=self.num_classes,\n",
    "                in_channels=3\n",
    "            )\n",
    "        \n",
    "        elif self.model_name == 'cnn_transformer':\n",
    "            return CNNTransformer(\n",
    "                num_classes=self.num_classes,\n",
    "                num_frames=30\n",
    "            )\n",
    "        \n",
    "        elif self.model_name == 'timesformer':\n",
    "            return TimeSformer(\n",
    "                num_classes=self.num_classes,\n",
    "                num_frames=30\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Model creation not implemented for: {self.model_name}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def list_available_models(cls):\n",
    "        \"\"\"Display all available model architectures.\"\"\"\n",
    "        print(\"Available Model Architectures:\")\n",
    "        for name, desc in cls.AVAILABLE_MODELS.items():\n",
    "            print(f\"- {name}: {desc}\")\n",
    "    \n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"Train for one epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = self.criterion(output, target)\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                logger.info(f'Train Batch: {batch_idx} Loss: {loss.item():.4f}')\n",
    "        \n",
    "        return {\n",
    "            'loss': total_loss / len(train_loader),\n",
    "            'accuracy': correct / total\n",
    "        }\n",
    "    \n",
    "    def validate(self, val_loader):\n",
    "        \"\"\"Validate model.\"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output = self.model(data)\n",
    "                \n",
    "                val_loss += self.criterion(output, target).item()\n",
    "                pred = output.argmax(dim=1)\n",
    "                correct += pred.eq(target).sum().item()\n",
    "                total += target.size(0)\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        # Update scheduler\n",
    "        self.scheduler.step(val_loss)\n",
    "        \n",
    "        return {\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': accuracy\n",
    "        }\n",
    "    \n",
    "    def train(self, train_loader, val_loader, num_epochs=50):\n",
    "        \"\"\"Full training loop with validation.\"\"\"\n",
    "        best_val_acc = 0\n",
    "        history = {\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': []\n",
    "        }\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            logger.info(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_metrics = self.train_epoch(train_loader)\n",
    "            history['train_loss'].append(train_metrics['loss'])\n",
    "            history['train_acc'].append(train_metrics['accuracy'])\n",
    "            \n",
    "            # Validate\n",
    "            val_metrics = self.validate(val_loader)\n",
    "            history['val_loss'].append(val_metrics['val_loss'])\n",
    "            history['val_acc'].append(val_metrics['val_accuracy'])\n",
    "            \n",
    "            # Save best model\n",
    "            if val_metrics['val_accuracy'] > best_val_acc:\n",
    "                best_val_acc = val_metrics['val_accuracy']\n",
    "                torch.save(self.model.state_dict(), \n",
    "                          f'checkpoints/{self.model_name}_best.pth')\n",
    "            \n",
    "            logger.info(\n",
    "                f\"Train Loss: {train_metrics['loss']:.4f} \"\n",
    "                f\"Train Acc: {train_metrics['accuracy']:.4f} \"\n",
    "                f\"Val Loss: {val_metrics['val_loss']:.4f} \"\n",
    "                f\"Val Acc: {val_metrics['val_accuracy']:.4f}\"\n",
    "            )\n",
    "        \n",
    "        return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CrossValidator:\n",
    "    \"\"\"Handle k-fold cross-validation.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_name: str,\n",
    "                 num_classes: int,\n",
    "                 n_splits: int = 5):\n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        self.n_splits = n_splits\n",
    "        self.kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    def run_cross_validation(self, dataset, batch_size=8):\n",
    "        \"\"\"Run k-fold cross-validation.\"\"\"\n",
    "        fold_results = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(self.kfold.split(dataset)):\n",
    "            logger.info(f\"\\nFold {fold + 1}/{self.n_splits}\")\n",
    "            \n",
    "            # Create data loaders for this fold\n",
    "            train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "            val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "            \n",
    "            train_loader = torch.utils.data.DataLoader(\n",
    "                dataset, batch_size=batch_size, sampler=train_sampler\n",
    "            )\n",
    "            val_loader = torch.utils.data.DataLoader(\n",
    "                dataset, batch_size=batch_size, sampler=val_sampler\n",
    "            )\n",
    "            \n",
    "            # Create and train model\n",
    "            trainer = ModelTrainer(self.model_name, self.num_classes)\n",
    "            history = trainer.train(train_loader, val_loader)\n",
    "            \n",
    "            fold_results.append({\n",
    "                'fold': fold + 1,\n",
    "                'best_val_acc': max(history['val_acc']),\n",
    "                'final_train_loss': history['train_loss'][-1],\n",
    "                'history': history\n",
    "            })\n",
    "            \n",
    "            # Clean up to free memory\n",
    "            del trainer\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return fold_results\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training and validation metrics.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(history['train_loss'], label='Train')\n",
    "    ax1.plot(history['val_loss'], label='Validation')\n",
    "    ax1.set_title('Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(history['train_acc'], label='Train')\n",
    "    ax2.plot(history['val_acc'], label='Validation')\n",
    "    ax2.set_title('Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_cv_results(cv_results):\n",
    "    \"\"\"Plot cross-validation results.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot accuracies across folds\n",
    "    accuracies = [result['best_val_acc'] for result in cv_results]\n",
    "    ax1.bar(range(1, len(accuracies) + 1), accuracies)\n",
    "    ax1.axhline(y=np.mean(accuracies), color='r', linestyle='--',\n",
    "                label=f'Mean: {np.mean(accuracies):.4f}')\n",
    "    ax1.set_title('Cross-Validation Accuracies')\n",
    "    ax1.set_xlabel('Fold')\n",
    "    ax1.set_ylabel('Best Validation Accuracy')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot training curves\n",
    "    for result in cv_results:\n",
    "        ax2.plot(result['history']['val_acc'],\n",
    "                label=f\"Fold {result['fold']}\")\n",
    "    ax2.set_title('Validation Accuracy Curves')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Model Architectures:\n",
      "- cnn_lstm: CNN-LSTM model with ResNet backbone\n",
      "- transformer: Video Transformer with self-attention\n",
      "- i3d: Inflated 3D ConvNet (I3D)\n",
      "- tgcn: Temporal Graph Convolutional Network\n",
      "- efficient: Memory-efficient Sign Language Network\n",
      "- cnn_transformer: Hybrid CNN-Transformer\n",
      "- timesformer: Time-Space Transformer\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m     plot_cv_results(cv_results)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 24\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     processor\u001b[38;5;241m.\u001b[39mpreprocess_videos()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Create dataloaders\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m data_loaders \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataloaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 2. Data Analysis\u001b[39;00m\n\u001b[1;32m     27\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m DataAnalyzer(PROCESSED_DIR)\n",
      "Cell \u001b[0;32mIn[8], line 40\u001b[0m, in \u001b[0;36mDataProcessor.create_dataloaders\u001b[0;34m(self, batch_size, num_workers)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessing_results.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     38\u001b[0m     data_info \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_data_loaders\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessed_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessed_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mframes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/yasir/D/sign language/wlasl_modified/src/data/loader.py:154\u001b[0m, in \u001b[0;36mcreate_data_loaders\u001b[0;34m(data_info, processed_dir, batch_size, num_workers, frame_cache_size)\u001b[0m\n\u001b[1;32m    152\u001b[0m split_data \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data_info:\n\u001b[0;32m--> 154\u001b[0m     split_data[\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msplit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m]\u001b[38;5;241m.\u001b[39mappend(item)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Create datasets\u001b[39;00m\n\u001b[1;32m    157\u001b[0m datasets \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    158\u001b[0m     split: MemoryEfficientDataset(\n\u001b[1;32m    159\u001b[0m         data\u001b[38;5;241m=\u001b[39mitems,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m split, items \u001b[38;5;129;01min\u001b[39;00m split_data\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    164\u001b[0m }\n",
      "\u001b[0;31mKeyError\u001b[0m: 'split'"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    # Configuration\n",
    "    DATA_DIR = Path('video')\n",
    "    PROCESSED_DIR = Path('processed')\n",
    "    NUM_CLASSES = 26  # Number of sign classes\n",
    "    BATCH_SIZE = 8\n",
    "    \n",
    "    # Create output directories\n",
    "    os.makedirs('checkpoints', exist_ok=True)\n",
    "    os.makedirs('processed', exist_ok=True)\n",
    "    \n",
    "    # List available models\n",
    "    ModelTrainer.list_available_models()\n",
    "    \n",
    "    # 1. Data Processing\n",
    "    processor = DataProcessor(DATA_DIR, PROCESSED_DIR)\n",
    "    \n",
    "    # Process videos if not already done\n",
    "    if not (PROCESSED_DIR / 'frames').exists():\n",
    "        processor.preprocess_videos()\n",
    "    \n",
    "    # Create dataloaders\n",
    "    data_loaders = processor.create_dataloaders(batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # 2. Data Analysis\n",
    "    analyzer = DataAnalyzer(PROCESSED_DIR)\n",
    "    stats = analyzer.analyze_dataset()\n",
    "    analyzer.plot_statistics(stats)\n",
    "    \n",
    "    # 3. Model Training and Evaluation\n",
    "    MODEL_NAME = 'cnn_lstm'  # Change this to try different architectures\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = ModelTrainer(MODEL_NAME, NUM_CLASSES)\n",
    "    \n",
    "    # Train model\n",
    "    history = trainer.train(\n",
    "        train_loader=data_loaders['train'],\n",
    "        val_loader=data_loaders['val'],\n",
    "        num_epochs=50\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # 4. Cross-Validation\n",
    "    validator = CrossValidator(MODEL_NAME, NUM_CLASSES)\n",
    "    cv_results = validator.run_cross_validation(data_loaders['train'].dataset)\n",
    "    \n",
    "    # Print and plot cross-validation results\n",
    "    accuracies = [result['best_val_acc'] for result in cv_results]\n",
    "    print(f\"\\nCross-validation results for {MODEL_NAME}:\")\n",
    "    print(f\"Mean accuracy: {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\")\n",
    "    \n",
    "    plot_cv_results(cv_results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
